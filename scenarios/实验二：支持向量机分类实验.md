# 实验二：支持向量机分类实验

## 一 实验目的

1. 了解支持向量机的基本原理，几何间隔函数间隔等基础概念
2. 了解支持向量机的优缺点和适用场景
3. 学会使用R语言建立SVM分类模型

## 二 实验原理

支持向量机（support vector machines，SVM\)是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming，不怕，附录有解释）的问题，也等价于正则化的合页损失函数（后面也有解释）的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。

下文将带您学习到支持向量机如何工作，以及如何利用R语言实现支持向量机分类算法。

### 线性分类

下面举个简单的例子，如下图所示，现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是 -1 ，另一边所对应的y全是1。

[![](/images/2-1_20171107135031.031.png)](http://www.x-lab.ac:13001/image/2-1_20171107135031.031.png)

这个超平面可以用分类函数![](/images/20131107201211968)表示，当f\(x\) 等于0的时候，x便是位于超平面上的点，而f\(x\)大于0的点对应 y=1 的数据点，f\(x\)小于0的点对应y=-1的点，如下图所示：

[![](/images/2-2_20171107135033.033.png)](http://www.x-lab.ac:13001/image/2-2_20171107135033.033.png)

### 函数间隔与几何间隔

在超平面w\*x+b=0确定的情况下，\|w\*x+b\|能够表示点x到距离超平面的远近，而通过观察w\*x+b的符号与类标记y的符号是否一致可判断分类是否正确，所以，可以用\(y\*\(w\*x+b\)\)的正负性来判定或表示分类的正确性。于此，我们便引出了函数间隔（functional margin）的概念。

定义函数间隔（用![](http://10.2.253.122:81/20140829135049264)表示）为：![](http://10.2.253.122:81/20131107201248921)

而超平面\(w，b\)关于T中所有样本点\(xi，yi\)的函数间隔最小值（其中，x是特征，y是结果标签，i表示第i个样本），便为超平面\(w, b\)关于训练数据集T的函数间隔：![](http://10.2.253.122:81/20131111154113734)= min![](http://10.2.253.122:81/20131111154113734)i  \(i=1，...n\)

但这样定义的函数间隔有问题，即如果成比例的改变w和b（如将它们改成2w和2b），则函数间隔的值f\(x\)却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还远远不够。事实上，我们可以对法向量w加些约束条件，从而引出真正定义点到超平面的距离--几何间隔（geometrical margin）的概念。假定对于一个点 x ，令其垂直投影到超平面上的对应点为 x0 ，w 是垂直于超平面的一个向量，![](http://10.2.253.122:81/20140829135315499)为样本x到分类间隔的距离，如下图所示：

[![](/images/2-3_20171107135035.035.png)](http://www.x-lab.ac:13001/image/2-3_20171107135035.035.png)

有![](http://10.2.253.122:81/20131107201720515)，其中\|\|w\|\|表示的是范数。

又由于 x0 是超平面上的点，满足 f\(x0\)=0 ，代入超平面的方程![](http://10.2.253.122:81/20131107201104906)即可算出：![](http://10.2.253.122:81/20131107201759093)（有的书上会写成把\|\|w\|\| 分开相除的形式，如本文参考文献及推荐阅读条目11，其中，\|\|w\|\|为w的二阶泛数）为了得到![](http://10.2.253.122:81/20140829135315499)的绝对值，令![](http://10.2.253.122:81/20140829135315499)乘上对应的类别 y，即可得出几何间隔（用![](http://10.2.253.122:81/20140829135609579)表示）的定义：![](http://10.2.253.122:81/20131107201919484)从上述函数间隔和几何间隔的定义可以看出：几何间隔就是函数间隔除以\|\|w\|\|，而且函数间隔y\*\(wx+b\) = y\*f\(x\)实际上就是\|f\(x\)\|，只是人为定义的一个间隔度量，而几何间隔\|f\(x\)\|/\|\|w\|\|才是直观上的点到超平面的距离。

### 最大间隔分类器

对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。这个间隔如下图中的gap / 2所示。

[![](/images/2-4_20171107135037.037.png)](http://www.x-lab.ac:13001/image/2-4_20171107135037.037.png)

通过由前面的分析可知：函数间隔不适合用来最大化间隔值，因为在超平面固定以后，可以等比例地缩放w的长度和b的值，这样可以使得![](http://10.2.253.122:81/20131107201211968)的值任意大，亦即函数间隔![](http://10.2.253.122:81/20131111154113734)可以在超平面保持不变的情况下被取得任意大。但几何间隔因为除上了![](http://10.2.253.122:81/20131111154326078)

，使得在缩放w和b的时候几何间隔![](http://10.2.253.122:81/20131111154137453)的值是不会改变的，它只随着超平面的变动而变动，因此，这是更加合适的一个间隔。所以，这里要找的最大间隔分类超平面中的“间隔”指的是几何间隔。

于是最大间隔分类器（maximum margin classifier）的目标函数可以定义为：[![](/images/2-9_20171107140109.009.png)](http://www.x-lab.ac:13001/image/2-9_20171107140109.009.png)同时需满足一些条件，根据间隔的定义，有

![](http://10.2.253.122:81/uploads/201210/25/1351141813_4166.jpg)  
其中，s.t.，即subject to的意思，它导出的是约束条件。回顾下几何间隔的定义![](http://10.2.253.122:81/20131107201919484)可知：如果令函数间隔![](http://10.2.253.122:81/20131111154113734)等于1，则有![](http://10.2.253.122:81/20131111154137453) = 1 / \|\|w\|\|且![](http://10.2.253.122:81/20140829140642940)，从而上述目标函数转化成了

![](http://10.2.253.122:81/uploads/201210/25/1351141837_7366.jpg)这个目标函数便是在相应的约束条件![](http://10.2.253.122:81/20140829140642940)下，最大化这个1/\|\|w\|\|值，而1/\|\|w\|\|便是几何间隔![](http://10.2.253.122:81/20131111154137453)。

如下图所示，中间的实线便是寻找到的最优超平面（Optimal Hyper Plane），其到两条虚线的距离相等，这个距离便是几何间隔![](http://10.2.253.122:81/20131111154137453)，两条虚线之间的距离等于2![](http://10.2.253.122:81/20131111154137453)，而虚线上的点则是支持向量。由于这些支持向量刚好在边界上，所以它们满足![](http://10.2.253.122:81/20131111155244218)（还记得我们把 functional margin 定为 1 了吗？上节中：处于方便推导和优化的目的，我们可以令![](http://10.2.253.122:81/20131111154113734)=1），而对于所有不是支持向量的点，则显然有![](http://10.2.253.122:81/20131111155205109)。![](http://10.2.253.122:81/20140829141714944)

### 算法源码
```

C_CLASSFICATION <- 0;
EPSILON_SVR <- 3;

is.bigmatrix.refer<-function(x)
{
    return( inherits(x, "BigMatrix.refer") );
}


svm <- function (x, ...)
    UseMethod ("svm")

svm.formula <- function (formula, data = NULL, ..., subset, na.action = na.omit, scale = TRUE)
{
    call <- match.call()
    if (!inherits(formula, "formula"))
        stop("method is only for formula objects")

    m <- match.call(expand.dots = FALSE)
    if (identical(class(eval.parent(m$data)), "matrix"))
        m$data <- as.data.frame(eval.parent(m$data))

    m$... <- NULL
    m$scale <- NULL
    m[[1]] <- as.name("model.frame")
    m$na.action <- na.action
    m <- eval(m, parent.frame())

    Terms <- attr(m, "terms")

    attr(Terms, "intercept") <- 0

    x <- model.matrix(Terms, m)

    y <- model.extract(m, "response")

    attr(x, "na.action") <- attr(y, "na.action") <- attr(m, "na.action")

    if (length(scale) == 1)
        scale <- rep(scale, ncol(x))

    if (any(scale))
    {
        remove <- unique(c(which(labels(Terms) %in%
                                 names(attr(x, "contrasts"))),
                           which(!scale) ) )
        scale <- !attr(x, "assign") %in% remove
    }

    ret <- svm.default (x, y, scale = scale, ..., na.action = na.action)
    ret$call <- call
    ret$call[[1]] <- as.name("svm")
    ret$terms <- Terms
    if (!is.null(attr(m, "na.action")))
        ret$na.action <- attr(m, "na.action")

    class(ret) <- c("svm.formula", class(ret))

    return (ret)
}

check_var_info <- function(x, y, sparse, type, scale, subset, na.action )
{
    x.scale <- y.scale <- NULL
    formula <- inherits(x, "svm.formula")

    nac <- attr(x, "na.action")

    ## scaling, and NA handling
    if (sparse)
    {
        scale <- rep(FALSE, ncol(x))
        if(!is.null(y)) na.fail(y)
        x <- SparseM::t(SparseM::t(x)) ## make shure that col-indices are sorted
    }
    else
    {
        ## na-handling for matrices
        if (!formula)
        {
            if (!missing(subset))
            {
                bigm.subset(x, subset);
                y <- y[subset];
            }

            if (is.null(y))
                nac <- bigm.naction(x, na.action)
            else
            {
                #df <- na.action(data.frame(y, x))
                #y <- df[,1]
                #x <- as.matrix(df[,-1])
                #nac <-
                #    attr(x, "na.action") <-
                #        attr(y, "na.action") <-
                #            attr(df, "na.action")

                sum.row <- rowSums(x) + as.numeric(y);
                sum.row <- na.action(sum.row);
                nac <- attr( y, "na.action") <- attr( sum.row, "na.action");
                bigm.naction( x, na.action, nac );
            }
        }

        ## scaling
        if (length(scale) == 1)
            scale <- rep(scale, ncol(x))

        if (any(scale))
        {
            #co <- !apply( x[, scale, drop = FALSE], 2, var )
            co <- !(unlist(lapply( 1:NCOL(x), function(i) {var(x[,i]);} ))[scale] )
            if (any(co))
            {
                warning(paste("Cannot scale data. Variable(s)", paste(sQuote(colnames(x[,scale,  drop = FALSE])[co]),   sep="", collapse=" and "), "constant. "))
                scale <- rep(FALSE, ncol(x))
            }
            else
            {
                x.scale <- bigm.scale( x, scale);
                # scale Y for regression
                if (is.numeric(y) && (type>2))
                {
                    y <- scale(y)
                    y.scale <- attributes(y)[c("scaled:center","scaled:scale")]
                    y <- as.vector(y)
                }
            }
        }
    }

    ## further parameter checks
    nr <- nrow(x);
    if ( length(y) != nr )
        stop("x and y don't match.")

    if (!is.vector(y) && !is.factor (y) )
        stop("y must be a vector or a factor.")

    #### C/C++ part of rgtTrain requires the Y is sorted by -1 and 1
    #### y.idx will be used later!

    y.org <- y;
    y.idx <- c();
    for( y0 in sort(unique(y) ) )
        y.idx <- c( y.idx, which( y == y0 ) );
    y <- y[y.idx];

    ### x <- x[y.idx,];
    if (sparse)
        x <- x[y.idx,]
    else
        bigm.subset( x, rows=y.idx, cols=NULL );

    lev <- NULL;
    ## in case of classification: transform factors into integers

    if( type < EPSILON_SVR )
    {
        if (is.factor(y))
        {
            lev <- levels(y)
        }
        else
        {
            if(any(as.integer(y) != y))
               stop("dependent variable has to be of factor or integer type for classification mode.")
            y <- as.factor(y)
            lev <- levels(y)
        }
    }

    nclass <- length(lev);

    return( list( nclass = nclass,
                lev   = lev,
                scale = scale,
                x.scale = x.scale,
                y.scale = y.scale,
                y.index = y.idx,
                y.orignal = y.org,
                nr      = nrow(x),
                nac     = nac,
                x       = x,
                y       = y ) );


}

svm.default <- function (x,
          y           = NULL,
          scale       = TRUE,
          type        = "C-classification",
          kernel      = "radial",
          degree      = 3,
          gamma       = if (is.vector(x)) 1 else 1 / ncol(x),
          coef0       = 0,
          cost        = 1,
          class.weights= NULL,
          tolerance   = 0.001,
          epsilon     = 0.1,
          shrinking   = TRUE,
          cross       = 0,
          probability = FALSE,
          fitted      = TRUE,
          rough.cross = 0,
          no.change.x = TRUE,
          verbose     = FALSE,
          ...,
          subset,
          na.action = na.omit)
{
    if (verbose) cat("cost=", cost, " gamma=", gamma, " epsilon=", epsilon, "coef0=", coef0, "degree=", degree, "\n");

    if ((is.vector(x) && is.atomic(x)))
        x <- t(t(x));

    if(inherits(x, "Matrix"))
    {
        requireNamespace("SparseM");
        requireNamespace("Matrix");
        x <- as(x, "matrix.csr");
    }

    if(inherits(x, "simple_triplet_matrix"))
    {
        requireNamespace("SparseM")
        ind <- order(x$i, x$j)
        x <- new("matrix.csr",
                 ra = x$v[ind],
                 ja = x$j[ind],
                 ia = as.integer(cumsum(c(1, tabulate(x$i[ind])))),
                 dimension = c(x$nrow, x$ncol))
    }

    if (sparse <- inherits(x, "matrix.csr") ||  inherits(x, "dgCMatrix" ))
    {
        requireNamespace("SparseM")
    }

    ## NULL parameters?
    if(is.null(degree)) stop(sQuote("degree"), " must not be NULL!")
    if(is.null(gamma)) stop(sQuote("gamma"), " must not be NULL!")
    if(is.null(coef0)) stop(sQuote("coef0"), " must not be NULL!")
    if(is.null(cost)) stop(sQuote("cost"), " must not be NULL!")
    if(is.null(epsilon)) stop(sQuote("epsilon"), " must not be NULL!")
    if(is.null(tolerance)) stop(sQuote("tolerance"), " must not be NULL!")
    if(is.null(shrinking)) stop("shrinking argument must not be NULL!")
    if(is.null(cross)) stop("cross argument must not be NULL!")
    if(is.null(probability)) stop("probability argument must not be NULL!")
    if(is.null(verbose)) stop("verbose argument must not be NULL!")

    ## only support C-classification
    if (is.null(type))
        type <- ifelse ( is.factor(y), "C-classification", "eps-regression");

    type.name <- type;
    type <- C_CLASSFICATION;
    if (type.name != "C-classification" && type.name != "eps-regression")
        stop("Rgtsvm only support C-classification and eps-regression!")
    else
        if(type.name == "eps-regression") type <- EPSILON_SVR;

    if (type != C_CLASSFICATION && length(class.weights) > 0)
    {
        class.weights <- NULL;
        warning(sQuote("class.weights"), " are set to NULL for regression mode. For classification, use a _factor_ for ", sQuote("y"),", or specify the correct ", sQuote("type"), " argument.");
    }

    ## kernel type
    kernel <- pmatch(kernel, c("linear",
                               "polynomial",
                               "radial",
                               "sigmoid"), 99) - 1;

    if (kernel > 10) stop("wrong kernel specification!");
    if (is.null(kernel)) stop("kernel argument must not be NULL!");
    if (is.null(sparse)) stop("sparse argument must not be NULL!");

    if( !sparse && ( class(x) %in% c("matrix", "data.frame") ) )
    {
        if( class(x) == "data.frame" ) x <- as.matrix(x);

        x.backup <- x;
        x <- attach.bigmatrix(x.backup);
    }

    # After this push, x$data will be scaled, so we have to save it to a RDS file(rds.save=TRUE).
    if ( inherits(x, "BigMatrix.refer") && no.change.x)
        bigm.push(x, rds.save=TRUE);

    var.info <- check_var_info( x, y, sparse, type, scale, subset, na.action )

    if ( cross > var.info$nr )
        stop(sQuote("cross"), " cannot exceed the number of observations!")

    biased <- ifelse(var.info$nclass<=2, TRUE, FALSE);

    if ( probability && !fitted) fitted <- TRUE;

    param <- list(type=type, type.name = type.name, kernel=kernel, degree=degree, gamma=gamma,
             coef0=coef0, cost=cost, tolerance=tolerance, epsilon=epsilon,
             shrinking=shrinking, cross=cross, rough.cross=rough.cross,
             sparse=sparse, probability=probability,
             biased = biased, fitted=fitted, nclass = var.info$nclass, class.weights= class.weights,
             verbose = verbose);

    x <- var.info$x;
    y <- var.info$y;
    var.info$y <- NULL;
    x.scale <- var.info$x.scale;
    y.scale <- var.info$y.scale;

    if( type == C_CLASSFICATION)
        cret <- gtsvmtrain.classfication.call( y, x, param, verbose=verbose );
    if( type == EPSILON_SVR)
        cret <- gtsvmtrain.regression.call( y, x, param, verbose=verbose );

    gtsvm.class <- ifelse( cret$nclasses==2, 1, cret$nclasses );
    if (missing(subset)) subset <- NULL;

    ret <- list (
                 call      = match.call(),
                 type.name = type.name,
                 type      = type,
                 kernel    = kernel,
                 cost      = cost,
                 degree    = degree,
                 gamma     = gamma,
                 coef0     = coef0,
                 class.weights = class.weights,

                 tolerance = tolerance,
                 epsilon   = epsilon,
                 sparse    = sparse,
                 scaled    = var.info$scale,
                 x.scale   = var.info$x.scale,
                 y.scale   = var.info$y.scale,
                 biased    = biased,
                 subset    = subset,

                 #number of classes
                 nclasses  = cret$nclasses,
                 levels    = var.info$lev,
                 # total number of sv
                 tot.nSV   = cret$nr,

                 # number of SV in diff. classes
                 nSV       = cret$nSV,

                 # labels of the SVs.
                 labels    = cret$label,

                 # SV matrix
                 SV       = cret$SV,

                 # indexes of sv in x
                 index     = cret$index,

                 ##constants in decision functions
                 rho       = cret$rho,

                 ## coefficiants of sv
                 coefs    = cret$coefs,

                 ## probability
                 compprob  = probability,
                 probA     = NULL,
                 probB     = NULL,

                 totalIter = cret$totalIter,
                 t.elapsed = cret$t.elapsed,
                 na.action = var.info$nac );

    if (cross > 0 )
    {
        if (inherits(x, "BigMatrix.refer") ) bigm.push(x);
        cross.ret <- cross_validation( y, x, param );
        if (inherits(x, "BigMatrix.refer") ) bigm.pop(x);

        ret$cross <- param$cross;
        ret$rough.cross <- param$rough.cross ;

        if ( type > 2)
        {
            scale.factor     <- if (any(scale)) crossprod(y.scale$"scaled:scale") else 1;
            ret$MSE          <- cross.ret$cresults * scale.factor;
            ret$tot.MSE      <- cross.ret$ctotal1  * scale.factor;
            ret$scorrcoeff   <- cross.ret$ctotal2;
        }
        else
        {
            ret$accuracies   <- cross.ret$cresults;
            ret$tot.accuracy <- cross.ret$ctotal1;
        }
    }

    ret$host <-  try(system("hostname", intern = TRUE),silent=T);
    class (ret) <- "gtsvm";

    if (fitted) {
        if(type == C_CLASSFICATION)
        {
            org.idx <- sort.int( var.info$y.index, index.return=T )$ix;

            ret$decision.values <- cret$decision[org.idx]
            ret$fitted <- as.factor(cret$predict[org.idx]);
            levels( ret$fitted ) <- var.info$lev;
            attr(ret$fitted, "decision.values") <- NULL;

            ret$fitted.accuracy <- length( which( ret$fitted == var.info$y.orignal ) )/length(y);

            if(param$nclass==2)
            {
                if(probability)
                {
                    prob <- svc_binary_train_prob( var.info$y.orignal, ret$decision.values );
                    ret$probA = prob$A;
                    ret$probB = prob$B;
                }
            }
            else
            {
                if(probability)
                {
                    org.idx <- sort.int( var.info$y.index, index.return=T )$ix;
                    ret$decision.values <- matrix( cret$decision,ncol=cret$nclasses )[org.idx, ];
                    ret$probA <- svc_one_again_all_train_prob( as.numeric(var.info$y.orignal), ret$decision.values );
                }
            }
        }
        else
        {
            org.idx <- sort.int( var.info$y.index, index.return=T )$ix;

            ret$decision.values <- matrix( cret$predict[ org.idx ], ncol=1) ;
            ret$fitted <- cret$predict[ org.idx ];
            y1 <- y[ org.idx ];

            if(!is.null(y.scale))
            {
                ret$fitted <- ret$fitted * y.scale$"scaled:scale" + y.scale$"scaled:center";
                y1 <- y1 * y.scale$"scaled:scale" + y.scale$"scaled:center";
            }

            y1.v <- ret$fitted;
            ret$fitted.MSE <- sum(( y1 - y1.v )^2, na.rm=T)/length(y1);
            ret$fitted.r2  <- ( length(y1)* sum(y1.v*y1, na.rm=T) - sum(y1.v, na.rm=T)*sum(y1, na.rm=T) )^2 / ( length(y1)*sum(y1.v^2, na.rm=T) - (sum(y1.v, na.rm=T))^2) / ( length(y1)*sum(y1^2, na.rm=T)- (sum(y1, na.rm=T))^2);
            ret$residuals <- (y1 - y1.v);

            if(probability)
               ret$probA <- eps_train_prob( y1, y1.v );

        }
    }

    ### restore x$data
    if(inherits(x, "BigMatrix.refer") && no.change.x) bigm.pop(x);

    ret
}

# Cross-Validation-routine from svm-train in R code
cross_validation <- function ( y, x, param )
{
    y.pre <- rep(NA, length(y));

    idx.shuffle <- sample(1:length(y));
    y <- y[ idx.shuffle ];
    if ( inherits(x, "BigMatrix.refer") ) bigm.subset( x, rows=idx.shuffle, cols=NULL ) else x <- x[idx.shuffle, ];

    breaks <- round(seq(1, length(y), length.out=param$cross+1));
    cresults <- c();

    for(i in 1:(length(breaks)-1))
    {
        idx.cross <- c(breaks[i]:breaks[i+1]);

        if( param$type == C_CLASSFICATION)
        {
            if ( inherits(x, "BigMatrix.refer") )
            {
                bigm.push(x);
                bigm.subset(x, rows = -idx.cross );
                x0 <- x;
            }
            else
                x0 <- x[-idx.cross,];

            sret <- gtsvmtrain.classfication.call( y[ -idx.cross ], x0, param, final.result=TRUE, verbose=FALSE, ignoreNoProgress=TRUE );
            if ( inherits(x, "BigMatrix.refer") ) bigm.pop(x);

            if ( inherits(x, "BigMatrix.refer") )
            {
                bigm.push(x);
                bigm.subset(x, rows = idx.cross );
                x0 <- x;
            }
            else
                x0 <- x[idx.cross,];

            pret <- gtsvmpredict.classfication.call( x0, param$sparse, sret, verbose=FALSE );
            if ( inherits(x, "BigMatrix.refer") ) bigm.pop(x);

            if ( sret$nclasses==2 )
                y.pre [idx.cross] <- levels(y)[as.factor(pret$ret)]
            else
            {
                ret2 <- matrix( pret$dec[ 1:(length(idx.cross)*param$nclass) ],
                                nrow = length(idx.cross), ncol= param$nclass  );
                y.pre [idx.cross] <- apply(ret2, 1, which.max);
            }

            cresults[i] = 100.0 * sum( as.integer(y.pre [idx.cross] == y[idx.cross]) )/length(idx.cross);
        }


        if( param$type == EPSILON_SVR)
        {
            if ( inherits(x, "BigMatrix.refer") )
            {
                bigm.push(x);
                bigm.subset(x, rows = -idx.cross );
                x0 <- x;
            }
            else
                x0 <- x[-idx.cross,];

            sret <- gtsvmtrain.regression.call( y[ -idx.cross ], x0, param, final.result=TRUE, verbose=FALSE, ignoreNoProgress=TRUE );
            if ( inherits(x, "BigMatrix.refer") ) bigm.pop(x);

            if (inherits(x, "BigMatrix.refer") )
            {
                bigm.push(x);
                bigm.subset(x, rows = idx.cross );
                x0 <- x;
            }
            else
                x0 <- x[idx.cross,];

            pret <- gtsvmpredict.regression.call( x0, param$sparse, sret, verbose=FALSE);
            if (inherits(x, "BigMatrix.refer") ) bigm.pop(x);

            y.pre [idx.cross] <- pret$ret;
            cresults[i] = sum(( y.pre[idx.cross] - y[idx.cross])^2 )/length(idx.cross)
        }

        if( param$rough.cross >0 && param$rough.cross==i)
            break;

    }

    if( any( which( is.na(y.pre) | is.na(y) ) ) )
    {
        y.sel <- !is.na(y.pre) & !is.na(y);
        y.pre <- y.pre[y.sel];
        y     <- y[y.sel];
    }

    if(param$type == EPSILON_SVR )
    {
        # MSE
        ctotal1 <- sum((y.pre-y)^2)/length(y);
        # R2
        ctotal2 <- (length(y)* sum(y.pre*y) - sum(y.pre)*sum(y) )^2 / ( length(y)*sum(y.pre^2) - (sum(y.pre))^2) / ( length(y)*sum(y^2)- (sum(y))^2);
    }
    else
    {
        ctotal1 <- 100.0 * sum(y.pre==y)/length(y);
        ctotal2 <- NA;
    }

    return(list(ctotal1=ctotal1, ctotal2=ctotal2, cresults=cresults));
}


predict.gtsvm <- function (object, newdata,
          decision.values = FALSE,
          probability = FALSE,
          verbose = FALSE,
          ...,
          na.action = na.omit)
{
    if (missing(newdata))
        return(fitted(object))

    if (object$tot.nSV < 1)
        stop("Model is empty!");

    if(inherits(newdata, "Matrix"))
    {
        requireNamespace("SparseM");
        requireNamespace("Matrix");
        newdata <- as(newdata, "matrix.csr");
    }

    if(inherits(newdata, "simple_triplet_matrix"))
    {
       requireNamespace("SparseM");
       ind <- order(newdata$i, newdata$j);
       newdata <- new("matrix.csr",
                      ra = newdata$v[ind],
                      ja = newdata$j[ind],
                      ia = as.integer(cumsum(c(1, tabulate(newdata$i[ind])))),
                      dimension = c(newdata$nrow, newdata$ncol));
    }

    sparse <- inherits(newdata, "matrix.csr");
    if (object$sparse || sparse)
        requireNamespace("SparseM");

    act <- NULL;
    if ((is.vector(newdata) && is.atomic(newdata)))
        newdata <- t(t(newdata));

    if (sparse)
        newdata <- SparseM::t(SparseM::t(newdata));

    preprocessed <- !is.null(attr(newdata, "na.action"))
    rowns <- if (!is.null(rownames(newdata)))
                rownames(newdata)
            else
                1:nrow(newdata);

    if( inherits(newdata, "BigMatrix.refer") ) bigm.push(newdata);

    if (!object$sparse)
    {
        if (inherits(object, "svm.formula"))
        {
            if(is.null(colnames(newdata)))
                colnames(newdata) <- colnames(object$SV);

            newdata <- na.action(newdata);
            act <- attr(newdata, "na.action");
            newdata <- model.matrix(delete.response(terms(object)),
                                    as.data.frame(newdata));
        }
        else
        {
            if( !inherits(newdata, "BigMatrix.refer") )
            {
                newdata <- na.action(as.matrix(newdata));
                act <- attr(newdata, "na.action");
            }
            else
            {
                act <- bigm.naction( newdata, na.action );
            }
        }
    }

    if (!is.null(act) && !preprocessed)
        rowns <- rowns[-act];

    if (any(object$scaled))
    {
        if( !inherits( newdata, "BigMatrix.refer") )
             newdata[,object$scaled] <-
                scale( newdata[, object$scaled, drop = FALSE], center = object$x.scale$"scaled:center", scale  = object$x.scale$"scaled:scale")
        else
            bigm.scale( newdata, object$scaled, center = object$x.scale$"scaled:center", scale  = object$x.scale$"scaled:scale" );
    }

    if (ncol(object$SV) != ncol(newdata))
        stop ("test data does not match model !");

    param <- list( decision.values = decision.values, probability = probability );

    # Call C/C++ interface to do predict
    if(object$type == C_CLASSFICATION)
        ret <- gtsvmpredict.classfication.call( newdata, sparse, object, param, verbose=verbose)
    else if(object$type == EPSILON_SVR)
        ret <- gtsvmpredict.regression.call( newdata, sparse, object, param, verbose=verbose)
    else
        stop("only 'C-classification' and 'eps-regression' are implemented in this package!");

    ret2 <- ret$ret;
    if (is.character(object$levels)) # classification: return factors
    {
        ret2 <- as.factor(ret$ret);
        levels( ret2 ) <- object$levels;
    }
    else if (any(object$scaled) && !is.null(object$y.scale)) # return raw values, possibly scaled back
        ret2 <- ret$ret * object$y.scale$"scaled:scale" + object$y.scale$"scaled:center";

    names(ret2) <- rowns;
    ret2 <- napredict(act, ret2);

    if (decision.values)
    {
        colns = c();
        #for (i in 1:(object$nclasses - 1))
        #    for (j in (i + 1):object$nclasses)
        #        colns <- c(colns,
        #                   paste(object$levels[object$labels[i]],
        #                         "/", object$levels[object$labels[j]],
        #                         sep = ""));

        if(object$nclasses==2)
            attr(ret2, "decision.values") <- napredict(act, matrix(ret$dec, nrow = nrow(newdata), ncol=1 ) )
        else
        {
            colns <- object$labels;
            attr(ret2, "decision.values") <-
                napredict(act, matrix(ret$dec, nrow = nrow(newdata), ncol=length(colns), byrow = FALSE, dimnames = list(rowns, colns) ) );
        }

    }

    if (probability && object$type < 2) {
        if (!object$compprob)
            warning("SVM has not been trained using `probability = TRUE`, probabilities not available for predictions.")
        else
        {
            if(object$nclasses==2)
                attr(ret2, "probabilities") <- napredict(act, svc_binary_predict_prob( ret$dec, object$probA, object$probB ) )
            else
            {
                colns <- object$labels;
                ydeci <- matrix(ret$dec, nrow = nrow(newdata), ncol=length(colns), byrow = FALSE, dimnames = list(rowns, colns) );
                ret$prob <- svc_one_again_all_predict_prob( ydeci, object$probA );
                colnames(ret$prob) <- object$labels;
                attr(ret2, "probabilities") <- napredict(act,ret$prob );
             }
        }
    }

    if( inherits(newdata, "BigMatrix.refer") ) bigm.pop(newdata);

    ret2
}

print.gtsvm <- function (x, ...)
{
    cat("\nCall:", deparse(x$call, 0.8 * getOption("width")), "\n", sep="\n");
    cat("Parameters:\n");
    cat("   SVM-Type: ", x$type.name, "\n");
    cat(" SVM-Kernel: ", c("linear",
                           "polynomial",
                           "radial",
                           "sigmoid")[x$kernel+1], "\n");

    cat("       cost: ", x$cost, "\n");
    if (x$kernel==1)
        cat("     degree: ", x$degree, "\n");
    cat("      gamma: ", x$gamma, "\n");
    if (x$kernel==1 || x$kernel==3)
        cat("     coef.0: ", x$coef0, "\n");

    cat("    tolerance: ", x$tolerance, "\n");
    cat(" time elapsed: ", x$t.elapsed[3], "\n\n");

    cat("\nNumber of Support Vectors: ", x$tot.nSV);
    cat("\n\n");

}

summary.gtsvm <- function(object, ...)
    structure(object, class="summary.gtsvm")

print.summary.gtsvm <- function (x, ...)
{
    print.gtsvm(x);

    cat(" (", x$nSV, ")\n\n");
    cat("\nNumber of Classes: ", x$nclasses, "\n\n");
    cat("Levels:", if(is.numeric(x$levels)) "(as integer)", "\n", x$levels);

    cat("\n\n");
}

#scale.data.frame <- function(x, center = TRUE, scale = TRUE)
#{
#    i <- sapply(x, is.numeric)
#    if (ncol(x[, i, drop = FALSE])) {
#        x[, i] <- tmp <- scale.default(x[, i, drop = FALSE], na.omit(center), na.omit(scale))
#        if(center || !is.logical(center))
#            attr(x, "scaled:center")[i] <- attr(tmp, "scaled:center")
#        if(scale || !is.logical(scale))
#            attr(x, "scaled:scale")[i]  <- attr(tmp, "scaled:scale")
#    }
#    x
#}

plot.gtsvm <- function(x, data, formula = NULL, fill = TRUE,
         grid = 50, slice = list(), symbolPalette = palette(),
         svSymbol = "x", dataSymbol = "o", ...)
{
    if (is.null(formula) && ncol(data) == 3)
    {
        formula <- formula(delete.response(terms(x)));
        formula[2:3] <- formula[[2]][2:3];
    }

    if (is.null(formula))
        stop("missing formula.");

    if (fill)
    {
        sub <- model.frame(formula, data);
        xr <- seq(min(sub[, 2]), max(sub[, 2]), length = grid);
        yr <- seq(min(sub[, 1]), max(sub[, 1]), length = grid);
        l <- length(slice);

        if (l < ncol(data) - 3)
        {
            slnames <- names(slice);
            slice <- c(slice, rep(list(0), ncol(data) - 3 - l));
            names <- labels(delete.response(terms(x)));
            names(slice) <- c(slnames, names[!names %in% c(colnames(sub), slnames)]);
        }

        for (i in names(which(sapply(data, is.factor))))
            if (!is.factor(slice[[i]])) {
                levs <- levels(data[[i]]);
                lev <- if (is.character(slice[[i]])) slice[[i]] else levs[1];
                fac <- factor(lev, levels = levs);
                if (is.na(fac))
                   stop(paste("Level", dQuote(lev), "could not be found in factor", sQuote(i)));
                slice[[i]] <- fac;
            }

        lis <- c(list(yr), list(xr), slice);
        names(lis)[1:2] <- colnames(sub);
        new <- expand.grid(lis)[, labels(terms(x))];
        preds <- predict(x, new);

        filled.contour(xr, yr,
               matrix(as.numeric(preds),
               nrow = length(xr), byrow = TRUE),
               plot.axes = {
                               axis(1)
                               axis(2)
                               colind <- as.numeric(model.response(model.frame(x, data)))
                               dat1 <- data[-x$index,]
                               dat2 <- data[x$index,]
                               coltmp1 <- symbolPalette[colind[-x$index]]
                               coltmp2 <- symbolPalette[colind[x$index]]
                               points(formula, data = dat1, pch = dataSymbol, col = coltmp1)
                               points(formula, data = dat2, pch = svSymbol, col = coltmp2)
                           },
               levels = 1:(length(levels(preds)) + 1),
               key.axes = axis(4, 1:(length(levels(preds))) + 0.5,
               labels = levels(preds),
               las = 3),
               plot.title = title(main = "SVM classification plot",
               xlab = names(lis)[2], ylab = names(lis)[1]), ...);
    }
    else {
        plot(formula, data = data, type = "n", ...);
        colind <- as.numeric(model.response(model.frame(x, data)));

        dat1 <- data[-x$index,];
        dat2 <- data[x$index,];
        coltmp1 <- symbolPalette[colind[-x$index]];
        coltmp2 <- symbolPalette[colind[x$index]];
        points(formula, data = dat1, pch = dataSymbol, col = coltmp1);
        points(formula, data = dat2, pch = svSymbol, col = coltmp2);

        invisible();
    }
}

predict.batch <- function (object, file.rds, decision.values = TRUE, probability = FALSE, verbose = FALSE, ..., na.action = na.omit)
{
    if (missing(file.rds))
        stop("No RDS files are specified.\n");

    if (object$tot.nSV < 1)
        stop("Model is empty!");

    if (object$sparse)
        requireNamespace("SparseM");

    x.count <- 0;
    rowns <- c();
    for(rds in file.rds)
    {
        newdata <- readRDS(rds);
        if (NCOL(newdata) != NCOL(object$SV))
            stop(paste("X data in RDS file doesn't have same number of feature vectors. File=", rds, sep=""));
        x.count <- x.count + NROW(newdata);
        rowns <- c( rowns, rownames(newdata));

        newdata <- na.action( as.matrix(newdata) );
        act <- attr(newdata, "na.action");
        if(!is.null(act))
            stop( paste( "Missing values in RDS file, file=", rds ) );
    }

    param <- list( decision.values = decision.values, probability = probability );

    # Call C/C++ interface to do predict
    if(object$type == C_CLASSFICATION)
        ret <- gtsvmpredict.classfication.batch.call( file.rds, x.count, object, param, verbose=verbose)
    else if(object$type == EPSILON_SVR)
        ret <- gtsvmpredict.regression.batch.call( file.rds, x.count, object, param, verbose=verbose)
    else
        stop("only 'C-classification' and 'eps-regression' are implemented in this package!");

    ret2 <- ret$ret;
    if (is.character(object$levels)) # classification: return factors
    {
        ret2 <- as.factor(ret$ret) ;
        levels( ret2 ) <- object$levels;
    }
    else if (any(object$scaled) && !is.null(object$y.scale)) # return raw values, possibly scaled back
        ret2 <- ret$ret * object$y.scale$"scaled:scale" + object$y.scale$"scaled:center";

    names(ret2) <- rowns;
    act <- NULL;

    if (decision.values)
    {
        colns = c();
        for (i in 1:(object$nclasses - 1))
            for (j in (i + 1):object$nclasses)
                colns <- c(colns,
                           paste(object$levels[object$labels[i]],
                                 "/", object$levels[object$labels[j]],
                                 sep = ""));

        attr(ret2, "decision.values") <-
            napredict(act, matrix(ret$dec, nrow = x.count, ncol=length(colns), byrow = TRUE, dimnames = list(rowns, colns) ) );
    }

    if (probability && object$type < 2)
    {
        if (!object$compprob)
            warning("SVM has not been trained using `probability = TRUE`, probabilities not available for predictions.")
        else
        {

            if(object$nclasses==2)
                attr(ret2, "probabilities") <- napredict(act, svc_binary_predict_prob( ret$dec, object$probA, object$probB ) )
            else
            {
                colns <- object$labels;
                ydeci <- matrix(ret$dec, nrow = nrow(newdata), ncol=length(colns), byrow = FALSE, dimnames = list(rowns, colns) );
                ret$prob <- svc_one_again_all_predict_prob( ydeci, object$probA );
                colnames(ret$prob) <- object$labels;
                attr(ret2, "probabilities") <- napredict(act,ret$prob );
             }
        }
    }

    ret2
}
```

## 三 实验步骤

让我们看一下如何使用支持向量机实现二元分类器，使用的数据是来自MASS包的cats数据集。在本例中你将尝试使用体重和心脏重量来预测一只猫的性别。我们拿数据集中20%的数据点，用于测试模型的准确性（在其余的80%的数据上建立模型）。

```
# Setup
library(e1071)
data(cats, package="MASS")
inputData <- data.frame(cats[, c (2,3)], response = as.factor(cats$Sex)) # response as factor
```

### 线性支持向量机

传递给函数svm\(\)的关键参数是kernel、cost和gamma。Kernel指的是支持向量机的类型，它可能是线性SVM、多项式SVM、径向SVM或Sigmoid SVM。Cost是违反约束时的成本函数，gamma是除线性SVM外其余所有SVM都使用的一个参数。还有一个类型参数，用于指定该模型是用于回归、分类还是异常检测。但是这个参数不需要显式地设置，因为支持向量机会基于响应变量的类别自动检测这个参数，响应变量的类别可能是一个因子或一个连续变量。所以对于分类问题，一定要把你的响应变量作为一个因子。

```
# linear SVM
svmfit <- svm(response ~ ., data = inputData, kernel = "linear", cost = 10, scale = FALSE) # linear svm, scaling turned OFF
print(svmfit)
plot(svmfit, inputData)
compareTable <- table (inputData$response, predict(svmfit))  # tabulate
mean(inputData$response != predict(svmfit)) # 19.44% misclassification error
```

[![](/images/2-6_20171107140305.005.png)](http://www.x-lab.ac:13001/image/2-6_20171107140305.005.png)

### 径向支持向量机

径向基函数作为一个受欢迎的内核函数，可以通过设置内核参数作为“radial”来使用。当使用一个带有“radial”的内核时，结果中的超平面就不需要是一个线性的了。通常定义一个弯曲的区域来界定类别之间的分隔，这也往往导致相同的训练数据，更高的准确度。

```
# radial SVM
svmfit <- svm(response ~ ., data = inputData, kernel = "radial", cost = 10, scale = FALSE) # radial svm, scaling turned OFF
print(svmfit)
plot(svmfit, inputData)
compareTable <- table (inputData$response, predict(svmfit))  # tabulate
mean(inputData$response != predict(svmfit)) # 18.75% misclassification error
```

[![](/images/2-7_20171107135043.043.png)](http://www.x-lab.ac:13001/image/2-7_20171107135043.043.png)

### 寻找最优参数

你可以使用tune.svm\(\)函数，来寻找svm\(\)函数的最优参数。

```
### Tuning
# Prepare training and test data
set.seed(100) # for reproducing results
rowIndices <- 1 : nrow(inputData) # prepare row indices
sampleSize <- 0.8 * length(rowIndices) # training sample size
trainingRows <- sample (rowIndices, sampleSize) # random sampling
trainingData <- inputData[trainingRows, ] # training data
testData <- inputData[-trainingRows, ] # test data
tuned <- tune.svm(response ~., data = trainingData, gamma = 10^(-6:-1), cost = 10^(1:2)) # tune
summary (tuned) # to select best gamma and cost
```
下面是结果
```
# Parameter tuning of 'svm':
#   - sampling method: 10-fold cross validation
#
# - best parameters:
#   gamma cost
# 0.001  100
#
# - best performance: 0.26
#
# - Detailed performance results:
#   gamma cost error dispersion
# 1  1e-06   10  0.36 0.09660918
# 2  1e-05   10  0.36 0.09660918
# 3  1e-04   10  0.36 0.09660918
# 4  1e-03   10  0.36 0.09660918
# 5  1e-02   10  0.27 0.20027759
# 6  1e-01   10  0.27 0.14944341
# 7  1e-06  100  0.36 0.09660918
# 8  1e-05  100  0.36 0.09660918
# 9  1e-04  100  0.36 0.09660918
# 10 1e-03  100  0.26 0.18378732
# 11 1e-02  100  0.26 0.17763883
# 12 1e-01  100  0.26 0.15055453
```

结果证明，当cost为100，gamma为0.001时产生最小的错误率。利用这些参数训练径向支持向量机。

```
svmfit <- svm (response ~ ., data = trainingData, kernel = "radial", cost = 100, gamma=0.001, scale = FALSE) # radial svm, scaling turned OFF
print(svmfit)
plot(svmfit, trainingData)
compareTable <- table (testData$response, predict(svmfit, testData))  # comparison table
mean(testData$response != predict(svmfit, testData)) # 27.59% misclassification error


F   M
F   6   3
M  1   19
```

### 网格图

一个2-色的网格图，能让结果看起来更清楚，它将图的区域指定为利用SVM分类器得到的结果的类别。在下边的例子中，这样的网格图中有很多数据点，并且通过数据点上的倾斜的方格来标记支持向量上的点。很明显，在这种情况下，有很多越过边界违反约束的点，但在SVM内部它们的权重都被降低了。


```
npointsin_grid = 60 # num grid points in a line
xaxisrange <- range (inputData[, 2]) # range of X axis
yaxisrange <- range (inputData[, 1]) # range of Y axis
Xgridpoints <- seq (from=xaxisrange[1], to=xaxisrange[2], length=npointsin_grid) # grid points along x-axis
Ygridpoints <- seq (from=yaxisrange[1], to=yaxisrange[2], length=npointsin_grid) # grid points along y-axis
allgridpoints <- expand.grid (Xgridpoints, Ygridpoints) # generate all grid points
names (allgridpoints) <- c("Hwt", "Bwt") # rename
allpointspredited <- predict(svmfit, allgridpoints) # predict for all points in grid
colorarray <- c("red", "blue")[as.numeric(allpointspredited)] # colors for all points based on predictions
plot (allgridpoints, col=colorarray, pch=20, cex=0.25) # plot all grid points
points (x=trainingData$Hwt, y=trainingData$Bwt, col=c("red", "blue")[as.numeric(trainingData$response)], pch=19) # plot data points
points (trainingData[svmfit$index, c (2, 1)], pch=5, cex=2) # plot support vectors
```

[![](/images/2-8_20171107135047.047.png)](http://www.x-lab.ac:13001/image/2-8_20171107135047.047.png)

## 四 常见问题：

1. **支持向量机如何工作？**  
   简单介绍下支持向量机是做什么的：

   假设你的数据点分为两类，支持向量机试图寻找最优的一条线（超平面），使得离这条线最近的点与其他类中的点的距离最大。有些时候，一个类的边界上的点可能越过超平面落在了错误的一边，或者和超平面重合，这种情况下，需要将这些点的权重降低，以减小它们的重要性。这种情况下，“支持向量”就是那些落在分离超平面边缘的数据点形成的线

2. **无法确定分类线（线性超平面）时该怎么办？**  
   此时可以将数据点投影到一个高维空间，在高维空间中它们可能就变得线性可分了。它会将问题作为一个带约束的最优化问题来定义和解决，其目的是为了最大化两个类的边界之间的距离。

3. **我的数据点多于两个类时该怎么办？**  
   此时支持向量机仍将问题看做一个二元分类问题，但这次会有多个支持向量机用来两两区分每一个类，直到所有的类之间都有区别。